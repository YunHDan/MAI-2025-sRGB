# -*- coding: utf-8 -*-
"""“Convert.ipynb”

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ntNaqiZUUW3J3fPsIGWfv-Qk65A_FrV5
"""

! pip install onnx onnx-tf tf2onnx

! pip install tensorflow==2.15.0 tf-keras==2.15.0 tensorflow-probability==0.22

import onnx
from onnx_tf.backend import prepare
import os
# # 加载 ONNX 模型
onnx_model = onnx.load("enhanceformer.onnx")


# 将 ONNX 模型转换为 TensorFlow SavedModel
tf_path = "saved_model"
tf_rep = prepare(onnx_model)
tf_rep.export_graph(tf_path)
print(f"Model exported to {tf_path}")

import tensorflow as tf

# 加载 SavedModel
saved_model_dir = "saved_model"

# 创建 TFLite 转换器
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

# 设置输入张量为动态尺寸
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]      # 启用TensorFlow Flex操作]
                                      #  tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops = True  # 允许自定义操作
# 设置输入张量的动态维度
converter.input_shapes = {"input": [1, -1, -1, 3]}  # [batch, height, width, channels]

# 转换模型
tflite_model = converter.convert()

# 保存 TFLite 模型
tflite_path = "model_none.tflite"
with open(tflite_path, "wb") as f:
    f.write(tflite_model)
print(f"TFLite model saved to {tflite_path}")

